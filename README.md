# NTAM Homework - Анализ резюме hh.ru

Этот проект состоит из двух частей:
1. Парсинг и обработка данных из CSV-файла
2. Обучение и использование регрессионной модели для предсказания зарплат

## Часть 1: Парсинг и обработка данных

### Требования
Загрузите сырые данные. Напишите пайплайн обработки таблицы, применив паттерн проектирования "цепочка ответственности". На выходе должен быть .npy файл, готовый к дальнейшему анализу.

### Использование
```bash
cd parsing_analysis
python app.py hh.csv
```

### Результат
Скрипт создаст три файла в папке parsing_analysis/:
* x_data.npy - матрица признаков
* y_data.npy - вектор целевой переменной
* processed_data.csv - полные обработанные данные

## Часть 2: Обучение и предсказание зарплат

### Требования
На основе своего "распаршенного" датасета hh.ru обучить регрессионную модель. Приложение должно хранить веса модели в папке resources репозитория и на вход принимать выход из пайплайна, которое Вы сделали.
Установка зависимостей

### Установка зависимостей
```bash
pip install -r requirements.txt
```

### Обучение модели
```bash
python simple_train.py
```
Этот скрипт:
1. Загрузит обработанные данные из parsing_analysis/processed_data.csv
2. Обучит линейную регрессионную модель на признаках без утечки данных
3. Сохранит модель и вспомогательные файлы в папку resources/

### Предсказание зарплат
```bash
python app.py parsing_analysis/x_data.npy
```

## Часть 3: Классификация уровня IT-специалистов (PoC)

### Требования
Построить модель, которая по данным резюме предсказывает уровень (junior/middle/senior).

### Структура
- `level_classification/` – все файлы, относящиеся к этой работе.
  - `config.py` – параметры и пути.
  - `prepare_data.py` – загрузка и фильтрация данных.
  - `train_model.py` – основной скрипт обучения и оценки.
  - `confusion_matrix.png` – пример визуализации (генерируется).
  - `feature_importance.csv` – важность признаков.

### Как запустить
```bash
python level_classification/train_model.py
```

### В процессе выполнения:
1. Загружаются данные из parsing_analysis/hh.csv (путь можно изменить в config.py).
2. Отфильтровываются только IT-резюме.
3. По полю с опытом работы формируется целевая переменная (level).
4. Строятся признаки: логарифм зарплаты, возраст, группа города, количество навыков, бинарные флаги технологий, тип занятости и график.
5. Обучается Random Forest с учётом дисбаланса классов.
6. Выводится отчёт о классификации (precision, recall, f1-score) и матрица ошибок.
7. Сохраняются:
    - модель resources/level_model.pkl
    - стандартизатор resources/level_scaler.pkl
    - кодировщик меток resources/level_encoder.pkl
    - график матрицы ошибок level_classification/confusion_matrix.png
    - важность признаков level_classification/feature_importance.csv


## Особенности реализации

### Часть 1 (Парсинг):
* Использован паттерн "Chain of Responsibility" в pipeline.py
* Обработка 10 различных полей из резюме
* Создание новых признаков (группы по возрасту, опыту, взаимодействия)
* Обработка пропущенных значений

### Часть 2 (Модель):
* Использование LinearRegression из scikit-learn
* Масштабирование признаков с помощью StandardScaler
* Устранение утечки данных (исключение salary, salary_log, high_salary из признаков)
* Сохранение модели и маппинга признаков в формате pickle
* Автоматическое сопоставление признаков из x_data.npy с признаками модели
* Валидация входных данных и обработка ошибок

### Лучшие практики
* Модульная структура проекта
* Обработка ошибок и валидация входных данных
* Логирование процесса выполнения
* Читаемый и документированный код
* Разделение кода на логические модули

### Зависимости
* pandas>=1.3.0
* numpy>=1.21.0
* scikit-learn>=1.0.0
* joblib>=1.1.0